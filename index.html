<!DOCTYPE html>
<html>
<head>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125103456-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125103456-1');
</script>
<meta charset="UTF8">
<link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="ushiku.css">
<link rel="shortcut icon" href="/assets/favicon.ico" type="image/vnd.microsoft.icon"/>
<link rel="icon" href="/assets/favicon.ico" type="image/vnd.microsoft.icon"/>
<title>Yoshitaka Ushiku's homepage</title>
</head>
<body>

<div>English/<a href="index_ja.html">Japanese</a></div>

<h1>Yoshitaka Ushiku</h1>

<div>
<img src="ushiku.jpg" style="width: 540px; max-width: 50%; height: auto;">
</div>

<h2>Talk Slides</h2>
<h3>Keynotes</h3>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/thoD2I4GyhT4he" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/YoshitakaUshiku/frontiers-of-vision-and-language-bridging-images-and-texts-by-deep-learning-81870137" title="Frontiers of Vision and Language: Bridging Images and Texts by Deep Learning" target="_blank">Frontiers of Vision and Language: Bridging Images and Texts by Deep Learning</a> </strong> from <strong><a href="https://www.slideshare.net/YoshitakaUshiku" target="_blank">Yoshitaka Ushiku</a></strong> </div>

<h2>Education</h2>
<dl>
<dt>2009</dt>
<dd>BS of Engineering (The Univeresity of Tokyo)</dd>
<dt>2011</dt>
<dd>MA of Information Science and Technology (The University of Tokyo)</dd>
<dt>2014</dt>
<dd>Ph.D. (The University of Tokyo)</dd>
</dl>

<h2>Profession</h2>
<dl>
<dt>Apr. 2013 - Mar. 2014</dt>
<dd>Research Fellow, Japan Society for Promotion of Science</dd>
<dt>June 2013 - Aug. 2013</dt>
<dd>Intern, Microsoft Research Redmond</dd>
<dt>Apr. 2014 - Mar. 2016</dt>
<dd>Research Scientist, NTT Communication Science Laboratories.</dd>
<dt>Apr. 2016 - Sep. 2018</dt>
<dd>Associate Professor, Department of Mechano-Informatics, Graduate School of Information Science and Technology, the University of Tokyo</dd>
<dt>June 2016 - Sep. 2018</dt>
<dd>Visiting Researcher, National Institute of Advanced Industrial Science and Technology (AIST)</dd>
<dt>Sep. 2016 - Sep. 2018</dt>
<dd>Collaborative Researcher, National Institute for Japanese Language and Linguistics (NINJAL)</dd>
<dt>Apr. 2018 - Sep. 2018</dt>
<dd>Technical Advisor, OMRON SINIC X Corporation (OSX)</dd>
<dt>Oct. 2018 - Mar. 2023</dt>
<dd>Principal Investigator, OMRON SINIC X Corporation (OSX)</dd>
<dt>Jan. 2019 - Oct. 2020</dt>
<dd>Chief Research Officer, Ridge-i Co., Ltd.</dd>
<dt>Apr. 2020 - Mar. 2023</dt>
<dd>Lecturer (part-time), Tsuda University</dd>
<dt>Nov. 2020 - Oct. 2021</dt>
<dd>External Director, Chief Research Officer, Ridge-i Co., Ltd.</dd>
<dt>July 2021 - </dt>
<dd><span class="mention">Lecturer (part-time), Tohoku University</span></dd>
<dt>Nov. 2021 - </dt>
<dd><span class="mention">Chief Research Officer, Ridge-i Co., Ltd.</span></dd>
<dt>Jan. 2022 - </dt>
<dd><span class="mention">Chief Executive Officer, <a href="ninebulls.html">Nine Bulls, LLC (In Japanese only)</a></span></dd>
<dt>Oct. 2023 - </dt>
<dd><span class="mention">Project Manager, National Institute of Advanced Industrial Science and Technology (AIST) Kakusei Project</span></dd>
<dt>Dec. 2023 - </dt>
<dd><span class="mention">Senior Visiting Scientist, RIKEN Center for Biosystems Dynamics Research</span></dd>
<dt>Apr. 2024 - </dt>
<dd><span class="mention">Vice President for Research, OMRON SINIC X Corporation (OSX)</span></dd>
<dt>Oct. 2024 - </dt>
<dd><span class="mention">Chief Executive Officer, <a href="https://www.nexascience.com">NexaScience, Inc. (In Japanese only)</a></span></dd>
</dl>

<h2>Activity</h2>

<h3>Society</h3>
<dl>
<dt>June 2018</dt>
<dd>International Conference on Multimedia Retrieval (ICMR) Publication Co-chairs</dd>
<dt>October 2019</dt>
<dd>International Conference on Computer Vision (ICCV) Workshop on Multi-Discipline Approach for Learning Concepts--Zero-Shot, One-Shot, Few-Shot and Beyond-- Organizer</dd>
<dt>November 2020</dt>
<dd>Asian Conference on Computer Vision (ACCV) Area Chair</dd>
<dt>December 2022</dt>
<dd>Asian Conference on Computer Vision (ACCV) Industrial Chair</dd>
<dt>August 2023</dt>
<dd>International Joint Conferences on Artificial Intelligence (IJCAI) Area Chair</dd>
<dt>December 2023</dt>
<dd>Neural Information Processing Systems (NeurIPS) Track Datasets and Benchmarks Area Chair</dd>
<dt>August 2024</dt>
<dd>International Joint Conferences on Artificial Intelligence (IJCAI) Area Chair</dd>
<dt>

<h3>Reviewer</h3>
<dl>
<dt>Conference</dt>
<dd>
<span class="conf">AAAI 2020</span>, 
<span class="conf">ACMMM 2013 2016 2018 2019</span>, 
<span class="conf">ACPR 2017</span>, 
<span class="conf">BMVC 2020</span>, 
<span class="conf">CVPR 2019 2020 2021 2022 2023 2024</span>, 
<span class="conf">ECCV 2020 (<a href="https://eccv2020.eu/outstanding-reviewers/">Outstanding Reviewer</a>) 2022</span>, 
<span class="conf">ICCV 2019 2021 2023</span>, 
<span class="conf">ICLR 2020 2021 2022 (<a href="https://iclr.cc/Conferences/2022/Reviewers">Highlighted Reviewer</a>) 2023 2024</span>, 
<span class="conf">ICML 2021 2022 2023 2024</span>, 
<span class="conf">IJCAI 2018 2019</span>, 
<span class="conf">NeurIPS 2020 2021 2022 2023 2024</span>, 
<span class="conf">PCM 2018</span>
</dd>
<dt>Journal</dt>
<dd>
<span class="journal">Advanced Robotics</span>, 
<span class="journal">Computer Speech and Language</span>, 
<span class="journal">IEEE Access</span>, 
<span class="journal">International Journal of Computer Vision</span>, 
<span class="journal">Neural Networks</span>, 
<span class="journal">Pattern Recognition Letters</span>, 
<span class="journal">Robotics and Automation Letters</span>, 
<span class="journal">Speech and Language Processing</span>, 
<span class="journal">The Visual Computer</span>
<span class="journal">Transactions on Affective Computing</span>, 
<span class="journal">Transactions on Audio</span>, 
<span class="journal">Transactions on Computer Vision and Applications</span>, 
<span class="journal">Transactions on Intelligent Systems and Technology</span>,
<span class="journal">Transactions on Multimedia</span>, 
<span class="journal">Transactions on Multimedia Computing, Communications, and Applications</span>, 
<span class="journal">Transactions on Pattern Analysis and Machine Intelligence</span>, 
<span class="journal">Transactions on Systems, Man and Cybernetics: Systems</span>.
</dd>
</dl>  

</dl>

<h2>Biography</h2>
<p>
Yoshitaka Ushiku is a Principal Investigator at OMRON SINIC X and Chief Research Officer at Ridge-i.
He received his B.E., M.A., and Ph.D. degrees from the University of Tokyo in 2009, 2011, and 2014, respectively.
In 2014, he joined NTT CS Labs, Japan, where he was involved in research on image recognition.
From 2016 to 2018, he was an Associate Professor at the University of Tokyo, Japan.
Currently, he is Vice President for Research at OMRON SINIC X and Chief Executive Officer at NexaScience, Inc. since 2024.
Since 2022, he is also the managing partner of Nine Bulls, LLC.
His research interests lie in cross-media understanding through machine learning, mainly for computer vision and natural language processing.
He received ACM Mutlimedia Grand Challenge Special Prize in 2011, ACM Multimedia Open Source Software Competition Honorable Mention in 2017, NVIDIA Pioneering Research Awards in 2017 and 2018, and NISTEP Nice Step Researcher in 2023.
</p>

<h2>Contact</h2>
<div>
<img src="mailaddr.png" width="427" height="32">
<a href="http://www.twitter.com/losnuevetoros"><img src="tw.png" width="32" height="32"></a>
<a href="http://www.facebook.com/yoshitaka.ushiku"><img src="fb.png" width="32" height="32"></a>
<a href="https://www.linkedin.com/in/losnuevetoros"><img src="li.png" width="38.4" height="32"></a>
<a href="http://www.slideshare.net/YoshitakaUshiku"><img src="ss.png" width="70.4" height="32"></a>
<a href="https://speakerdeck.com/yushiku"><img src="sd.png" width="32" height="32"></a>
<a href="https://scholar.google.co.jp/citations?user=kxUld9MAAAAJ&hl=ja"><img src="gs.png" width="32" height="32"></a>
<a href="https://orcid.org/0000-0002-9014-1389"><img src="oi.png" width="32" height="32"></a>
</div>

<h2>Papers</h2>
<h3>Journal (refereed)</h3>
<ol>
<li>Yusaku Nakajima, Kai Kawasaki, Yasuo Takeichi, Masashi Hamaya, Yoshitaka Ushiku, and Kanta Ono. Force-Controlled Robotic Mechanochemical Synthesis. <span class="journal">Digital Discovery</span>, Vol. 3, pp.2130-2136, 2024.</li>
<li>Shigeo Yoshida, Yuki Koyama, and Yoshitaka Ushiku. Towards AI-Mediated Avatar-Based Telecommunication: Investigating Visual Impression of Switching Between User-and AI-Controlled Avatars in Video Chat. <span class="journal">IEEE Access</span>, Vol. 12, pp. 113372-113383, 2024.</li>
<li>Taichi Nishimura, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori. Recipe Generation from Unsegmented Cooking Videos. <span class="journal">ACM Transactions on Multimedia Computing, Communications and Applications</span>, 2024.</li>
<li>Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, and Yoshitaka Ushiku. Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. <span class="journal">Data-centric Machine Learning Research</span>, Vol. 1, No. 3, pp. 1-38, 2024.</li>
<li>Naoya Chiba, Yuta Suzuki, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, Kotaro Saito, and Kanta Ono. Neural structure fields with application to crystal structure autoencoders. <span class="journal">Communications Materials</span>, Vol.4, No.1, p.106, 2023.</li>
<li>Kazuhiro Ogata, Reo Gakumi, Atsushi Hashimoto, Yoshitaka Ushiku, and Shigeo Yoshida. The influence of Bouba-and Kiki-like shape on perceived taste of chocolate pieces. <span class="journal">Frontiers in Psychology</span>, Vol.14, 2023.</li>
<li>Taichi Nishimura, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori. State-aware video procedural captioning. <span class="journal">Multimedia Tools and Applications</span>, Vol.82, pp.37273-37301, 2023.</li>
<li>Yutaka Maruyama, Ryo Igarashi, Yoshitaka Ushiku, and Ayori Mitsutake. Analysis of Protein Folding Simulation with Moving Root Mean Square Deviation. <span class="journal">Journal of Chemical Information and Modeling</span>, Vol.63, No.5, pp.1529-1541, 2023.</li>
<li>Yuta Suzuki, Tatsunori Taniai, Kotaro Saito, Yoshitaka Ushiku, and Kanta Ono. Self-supervised learning of materials concepts from crystal structures via deep neural networks. <span class="journal">Machine Learning: Science and Technology</span>, pp.2632-2153, 2022.</li>
<li>Mutsuki Nakahara, Mai Nishimura, Yoshitaka Ushiku, Takayuki Nishio, Kazuki Maruta, Yu Nakayama, and Daisuke Hisano. Edge Computing-Assisted DNN Image Recognition System With Progressive Image Retransmission. <span class="journal">IEEE Access</span>, Vol.10, pp.91253-91262, 2022.</li>
<li>Takehiko Ohkawa, Takuma Yagi, Atsushi Hashimoto, Yoshitaka Ushiku, and Yoichi Sato. Foreground-Aware Stylization and Consensus Pseudo-Labeling for Domain Adaptation of First-Person Hand Segmentation. <span class="journal">IEEE Access</span>, Vol.9, pp.94644-94655, 2021.</li>
<li>Taichi Nishimura, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, Yoko Yamakata, and Shinsuke Mori. Structure-Aware Procedural Text Generation From an Image Sequence. <span class="journal">IEEE Access</span>, Vol.9, pp.2125-2141, 2021.</li>
<li>Hiroaki Minoura, Ryo Yonetani, Mai Nishimura, and Yoshitaka Ushiku. Crowd Density Forecasting by Modeling Patch-Based Dynamics. <span class="journal">IEEE Robotics and Automation Letters</span>, Vol.6, No.2, pp.287-294, 2021.</li>
<li>Yusuke Mori, Hiroaki Yamane, Yoshitaka Ushiku, and Tatsuya Harada. How narratives move your mind: A corpus of shared-character stories for connecting emotional flow and interestingness. <span class="journal">Information Processing &amp; Management</span>, Vol.56, No.5, pp.1865-1879, 2019.</li>
</ol>
<h3>International Conference (refereed)</h3>
<ol>
<li>Atsushi Hashimoto, Koki Maeda, Tosho Hirasawa, Jun Harashima, Leszek Rybicki, Yusuke Fukasawa, and Yoshitaka Ushiku. COM Kitchens: An Unedited Overhead-view Procedural Videos Dataset a Vision-Language Benchmark. European Conference on Computer Vision (<span class="conf">ECCV</span>), 2024.</li>
<li>Keisuke Shirai, Cristian C Beltran-Hernandez, Masashi Hamaya, Atsushi Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka Ushiku, and Shinsuke Mori. Vision-language interpreter for robot task planning. IEEE International Conference on Robotics and Automation (<span class="conf">ICRA</span>), 2024.</li>
<li>Sota Miyamoto, Takuma Yagi, Yuto Makimoto, Mahiro Ukai, Yoshitaka Ushiku, Atsushi Hashimoto, and Nakamasa Inoue. PolarDB: Formula-Driven Dataset for Pre-Training Trajectory Encoders. IEEE International Conference on Acoustics, Speech and Signal Processing (<span class="conf">ICASSP</span>), 2024.</li>
<li>Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, and Kanta Ono. Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding. International Conference on Learning Representations (<span class="conf">ICLR</span>), 2024.</li>
<li>Yusaku Nakajima, Masashi Hamaya, Kazutoshi Tanaka, Takafumi Hawai, Felix von Drigalski, Yasuo Takeichi, Yoshitaka Ushiku, and Kanta Ono. Robotic Powder Grinding with Audio-Visual Feedback for Laboratory Automation in Materials Science. IEEE/RSJ International Conference on Intelligent Robots and Systems (<span class="conf">IROS</span>), 2023.</li>
<li>Yusaku Nakajima, Masashi Hamaya, Yuta Suzuki, Takafumi Hawai, Felix von Drigalski, Kazutoshi Tanaka, Yoshitaka Ushiku, and Kanta Ono. Robotic Powder Grinding with a Soft Jig for Laboratory Automation in Material Science. IEEE/RSJ International Conference on Intelligent Robots and Systems (<span class="conf">IROS</span>), 2022.</li>
<li>Keisuke Shirai, Atsushi Hashimoto, Taichi Nishimura, Hirotaka Kameko, Shuhei Kurita, Yoshitaka Ushiku, and Shinsuke Mori. Visual Recipe Flow: A Dataset for Learning Visual State Changes of Objects with Recipe Flows. International Conference on Computational Linguistics (<span class="conf">COLING</span>), 2022.</li>
<li>Taichi Nishimura, Atsushi Hashimoto, Yoshitaka Ushiku, Hirotaka Kameko, and Shinsuke Mori. State-aware Video Procedural Captioning. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), 2021.</li>
<li>Mutsuki Nakahara, Daisuke Hisano, Mai Nishimura, Yoshitaka Ushiku, Kazuki Maruta, and Yu Nakayama. Retransmission Edge Computing System Conducting Adaptive Image Compression Based on Image Recognition Accuracy. IEEE Vehicular Technology Conference (<span class="conf">VTC-Fall</span>), 2021. </li>
<li>Qing Yu, Atsushi Hashimoto, and Yoshitaka Ushiku. Divergence Optimization for Noisy Universal Domain Adaptation. The IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2021.</li>
<li>Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, and Yuji Matsumoto. Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning. The Conference of the European Chapter of the Association for Computational Linguistics (<span class="conf">EACL</span>), 2021.</li>
<li>Taichi Nishimura, Suzushi Tomori, Hayato Hashimoto, Atsushi Hashimoto, Yoko Yamakata, Jun Harashima, Yoshitaka Ushiku, and Shinsuke Mori. Visual Grounding Annotation of Recipe Flow Graph. Language Resources and Evaluation Conference (<span class="conf">LREC</span>), 2020.</li>
<li>Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada. Class-distinct and class-mutual image generation with GANs. British Machine Vision Conference (<span class="conf">BMVC</span>), 2019.</li>
<li>Mikihiro Tanaka, Takayuki Itamochi, Kenichi Narioka, Ikuro Sato, Yoshitaka Ushiku, and Tatsuya Harada. Generating Easy-to-Understand Referring Expressions for Target Identifications. The IEEE International Conference on Computer Vision (<span class="conf">ICCV</span>),	2019.
<li>Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada. Label-noise robust generative adversarial networks. The IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2019.</li>
<li>Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distribution alignment for adaptive object detection. The IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2019</li>
<li>Yang Li, Yoshitaka Ushiku, and Tatsuya Harada. Pose Graph Optimization for Unsupervised Monocular Visual Odometry. International Conference on Robotics and Automation (<span class="conf">ICRA</span>), 2019.</li>
<li>Akane Iseki, Yusuke Mukuta, Yoshitaka Ushiku, and Tatsuya Harada. Estimating the causal effect from partially observed time series. The AAAI Conference on Artificial Intelligence (<span class="conf">AAAI</span>), 2019.</li>
<li>Kohei Uehara, Antonio Tejero-de-Pablos, Yoshitaka Ushiku, Tatsuya Harada. Visual Question Generation for Class Acquisition of Unknown Objects. The 15th European Conference on Computer Vision (<span class="conf">ECCV</span>), 2018.</li>
<li>Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada. Open Set Domain Adaptation by Backpropagation. The 15th European Conference on Computer Vision (<span class="conf">ECCV</span>), 2018.</li>
<li>Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada. Customized Image Narrative Generation via Interactive Visual Question Generation and Answering. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, Tatsuya Harada. Viewpoint-aware Video Summarization. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada. Neural 3D Mesh Renderer. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada. Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (oral presentation)</li>
<li>Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada. Between-class Learning for Image Classification. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018..</li>
<li>Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko. Adversarial Dropout Regularization. The 6th International Conference on Learning Representations (<span class="conf">ICLR</span>), 2018.</li>
<li>Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada. Learning from Between-class Examples for Deep Sound Recognition. The 6th International Conference on Learning Representations (<span class="conf">ICLR</span>), 2018.</li>
<li>Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada. Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture. AAAI Conference on Artificial Intelligence (<span class="conf">AAAI</span>), 2018. (oral presentation)</li>
<li>Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Alternating Circulant Random Features for Semigroup Kernels. AAAI Conference on Artificial Intelligence (<span class="conf">AAAI</span>), 2018.</li>
<li>Masatoshi Hidaka, Yuichiro Kikura, Yoshitaka Ushiku, Tatsuya Harada. WebDNN: Fastest DNN Execution Framework on Web Browser. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), Open Source Software Competition, pp.1213-1216, 2017.</li>
<li>Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada. Spatio-temporal Person Retrieval via Natural Language Queries. IEEE International Conference on Computer Vision (<span class="conf">ICCV</span>), 2017.</li>
<li>Qishen Ha, Kohei Watanabe, Takumi Karasawa, Yoshitaka Ushiku, Tatsuya Harada. MFNet: Towards Real-Time Semantic Segmentation for Autonomous Vehicles with Multi-Spectral Scenes. IEEE/RSJ International Conference on Intelligent Robots and Systems (<span class="conf">IROS</span>), 2017.</li>
<li>Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric Tri-training for Unsupervised Domain Adaptation. International Conference on Machine Learning (<span class="conf">ICML</span>), pp.2988-2997, 2017.</li>
<li>Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. DualNet: Domain-Invariant Network for Visual Question Answering. IEEE International Conference on Multimedia and Expo (<span class="conf">ICME</span>), pp.829-834, 2017. (oral presentation)</li>
<li>Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset. British Machine Vision Conference (<span class="conf">BMVC</span>), pp.53.1-53.12, 2016.</li>
<li>Yoshitaka Ushiku, Masataka Yamaguchi, Yusuke Mukuta, and Tatsuya Harada. Common subspace for model and similarity: Phrase learning for caption generation from images. IEEE International Conference on Computer Vision (<span class="conf">ICCV</span>), pp.2668-2676, 2015. (acceptance rate: 30.9%)</li>
<li>Yoshitaka Ushiku, Masatoshi Hidaka, and Tatsuya Harada. Three guidelines of online learning for large-scale visual recognition. IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), pp.3574-3581, 2014. (acceptance rate: 29.9%)</li>
<li>Asako Kanezaki, Shogo Inaba, Yoshitaka Ushiku, Yukihiko Yamashita, Hiroaki Muraoka, Yasuo Kuniyoshi, and Tatsuya Harada. Hard negative classes for multiple object detection. IEEE International Conference on Robotics and Automation (<span class="conf">ICRA</span>), pp.3066-3073, 2014.</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Efficient Image Annotation for Automatic Sentence Generation. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), pp.549-558, 2012. (full paper, acceptance rate: 20.2%)</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Understanding Images with Natural Sentences. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), Multimedia Grand Challenge, pp.679-682, 2011. (<span class="mention">Special Prize on the Best Application of a Theoretical Framework</span>) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/ACMMM2011_MGC_ushiku.pdf">pdf</a>]</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Automatic Sentence Generation from Images. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), pp.1533-1536, 2011. (short, acceptance rate: usually 30%) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/ACMMM2011_ushiku.pdf">pdf</a>]</li>
<li>Tatsuya Harada, Yoshitaka Ushiku, Yuya Yamashita, and Yasuo Kuniyoshi. Discriminative Spatial Pyramid. IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), pp.1617-1624, 2011. (acceptance rate: 26.4%) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/CVPR2011_harada.pdf">pdf</a>]</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Improvement of Image Similarity Measures for Image Browsing and Retrieval Via Latent Space Learning between Images and Long Texts. IEEE International Conference on Image Processing (<span class="conf">ICIP</span>), pp.2365-2368, 2010. [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2010/ICIP2010_ushiku.pdf">pdf</a>]</li>
</ol>
<h3>International Conference (unrefereed, demo or workshop)</h3>
<ol>
<li>Rintaro Yanagi, Atsushi Hashimoto, Naoya Chiba, and Yoshitaka Ushiku. Reference-based dense pose estimation via Partial 3D Point Cloud Matching. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>) Demo Paper Track, 2023.</li>
<li>Kuniaki Saito, Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Deep Modality Invariant Adversarial Network for Shared Representation Learning. The 16th International Conference on Computer Vision Workshop on Transferring and Adapting Source Knowledge in Computer Vision (<span class="conf">ICCV</span>, Workshop), 2017.</li>
<li>Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Spatial-Temporal Weighted Pyramid using Spatial Orthogonal Pooling. The 16th International Conference on Computer Vision Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (<span class="conf">ICCV</span>, Workshop), 2017.</li>
<li>Takumi Karasawa, Kohei Watanabe, Qishen Ha, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, Tatsuya Harada. Multispectral Object Detection for Autonomous Vehicles. The 25th Annual ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), 2017, (workshop).</li>
<li>Yoshitaka Ushiku, Hiroshi Muraoka, Sho Inaba, Teppei Fujisawa, Koki Yasumoto, Naoyuki Gunji, Takayuki Higuchi, Yuko Hara, Tatsuya Harada, and Yasuo Kuniyoshi. ISI at ImageCLEF 2012: Scalable System for Image Annotation. the 3rd Conference and Labs of the Evaluation Forum (<span class="conf">CLEF</span> 2012), pp.1-12, 2012.</li>
</ol>
<h3>Technical Report</h3>
<ol>
<li>Shoji Yamamoto, Antonio Tejero-de-Pablos, Yoshitaka Ushiku, and Tatsuya Harada. Conditional Video Generation Using Action-Appearance Captions. arXiv, 1812.01261, 2018.</li>
<li>Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA). arXiv, 1609.06657, 2016.</li>
</ol>

<h3>Domestic Journal (refereed, In Japanese)</h3>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic papers.
</p>
<h3>Domestic Conference (refereed, In Japanese)</h3>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic papers.
</p>
<h3>Domestic Conference (unrefereed, In Japanese)</h3>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic papers.
</p>

<h2>Books</h2>
<ol>
<li>Yoshitaka Ushiku. Long Short-Term Memory. In: Ikeuchi K. (eds) Computer Vision. Springer, 2020.</li>
</ol>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic books.
</p>
  
<h2>Invited Talks</h2>
<ol>
<li>Yoshitaka Ushiku. Towards a symbiosis between AI and humans: The State-of-the-art. Director General of Intellectual Property, online, Indonesia, 2022/01/13.</li>
<li>Yoshitaka Ushiku. Challenges of Integrating Vision and Language. International Display Workshops, online, Japan, 2021/12/01.</li>
<li>Yoshitaka Ushiku. Towards a symbiosis between AI and humans: The State-of-the-art. Intellectual Property Office of Vietnam, online, Vietnam, 2021/11/17.</li>
<li>Yoshitaka Ushiku. Multimodal Understanding: Vision and Language, and its Beyond. International Workshop on Frontiers of Computer Vision, Daegu, Korea, 2021/02/22.</li>
<li>Yoshitaka Ushiku. Deep Learning for Natural Language Processing and Computer Vision. Tutorial on Asian Conference on Machine Learning, Nagoya, Japan, 2019/11/17.</li>
<li>Yoshitaka Ushiku. Frontiers of Vision and Language: Bridging Images and Texts by Deep Learning. Workshop of Machine Learning under International Conference on Document Analysis and Recognition, Kyoto, Japan, 2017/11/11.</li>
<li>Yoshitaka Ushiku. Recognize, Describe, and Generate: Introduction of Recent Work at MIL. GPU Technology Conference, San Jose, CA, 2017/05/11.</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Efficient Image Annotation for Automatic Sentence Generation. Greater Tokyo Area Multimedia/Vision Workshop, Tokyo, Japan, 2012/08/30.</li>
</ol>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic talks.
</p>

<h2>Awards and Competitions</h2>
<ol>
<li>2023. <span class="award">Nice Step Researcher</span>. National Institute of Science and Technology Policy.</li>
<li>2018. <span class="award">NVIDIA Pioneering Research Awards</span> for Neural 3D Mesh Renderer.</li>
<li>2017. <span class="award">NVIDIA Pioneering Research Awards</span> for Asymmetric Tri-training for unsupervised domain adoptation.</li>
<li>2017. <span class="award">Honorable Mention</span>. ACM Multimedia Open Source Software Competition.</li>
<li>2016. <span class="award">First place</span> in the abstract image task. Visual Question Answering Challenge 2016.</li>
<li>2012. <span class="award">First place</span> in the fine-grained classification task, <span class="award">second place</span> in the classification task. Large Scale Visual Recognition Challenge 2012 (ILSVRC2012).</li>
<li>2011. <span class="award">Special Prize on the Best Application of a Theoretical Framework</span>. ACM Mutlimedia Grand Challenge.</li>
<li>2011. <span class="award">Third place</span> in the classification task, <span class="award">second place</span> in the detection task. Large Scale Visual Recognition Challenge 2011 (ILSVRC2011).</li>
<li>2010. <span class="award">Third place</span>. Large Scale Visual Recognition Challenge 2010 (ILSVRC2010).</li>
</ol>
<p>
Go to <a href="index_ja.html">japanese page</a> for domestic awards.
</p>

</body>
</html>
