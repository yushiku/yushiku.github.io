<!DOCTYPE html>
<html>
<head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-42274304-1', 'u-tokyo.ac.jp');
  ga('send', 'pageview');

</script>
<meta charset="UTF8">
<link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="ushiku.css">
<link rel="shortcut icon" href="/assets/favicon.ico" type="image/vnd.microsoft.icon"/>
<link rel="icon" href="/assets/favicon.ico" type="image/vnd.microsoft.icon"/>
<title>Yoshitaka Ushiku's homepage</title>
</head>
<body>

<div><a href="index.html">English</a>/Japanese</div>

<h1>牛久 祥孝</h1>

<div>
<img src="ushiku.jpg" width="540" height="360">
</div>

<h2>定常的コンテンツ</h2>
<h3>講演資料</h3>
<iframe src="//www.slideshare.net/slideshow/embed_code/key/97gT959XSUp2M7" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

<div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/YoshitakaUshiku/ss-57148161" title="画像キャプションの自動生成" target="_blank">画像キャプションの自動生成</a> </strong> from <strong><a target="_blank" href="//www.slideshare.net/YoshitakaUshiku">牛久 祥孝</a></strong> </div>

<iframe src="//www.slideshare.net/slideshow/embed_code/key/DROkd4XqzWRkJW" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

<div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/YoshitakaUshiku/deep-learning-73499744" title="Deep Learning による視覚×言語融合の最前線" target="_blank">Deep Learning による視覚×言語融合の最前線</a> </strong> from <strong><a target="_blank" href="//www.slideshare.net/YoshitakaUshiku">牛久 祥孝</a></strong> </div>

<h3>研究小ネタ</h3>
<ul>
<li><a href="how_to_write_references.html">参考文献の書き方</a></li>
</ul>

<h2>学歴</h2>
<dl>
<dt>2009年3月</dt>
<dd>東京大学 工学部 機械情報工学科 卒業</dd>
<dt>2011年3月</dt>
<dd>東京大学 大学院情報理工学系研究科 知能機械情報学専攻 修士課程修了</dd>
<dt>2014年3月</dt>
<dd>東京大学 大学院情報理工学系研究科 知能機械情報学専攻 博士課程修了 博士(情報理工学)</dd>
</dl>

<h2>職歴</h2>
<dl>
<dt>2013年4月-2014年3月</dt>
<dd>日本学術振興会 特別研究員（DC2）</dd>
<dt>2013年6月-2013年8月</dt>
<dd>Microsoft Research Redmond Intern</dd>
<dt>2014年4月-2016年3月</dt>
<dd>日本電信電話株式会社 コミュニケーション科学基礎研究所 研究員</dd>
<dt>2016年4月-現在</dt>
<dd>東京大学 大学院情報理工学系研究科 知能機械情報学専攻 講師</dd>
<dt>2016年9月-現在</dt>
<dd>国立研究開発法人産業技術総合研究所 協力研究員</dd>
<dt>2016年12月-現在</dt>
<dd>大学共同利用機関法人 人間文化研究機構 国立国語研究所 共同研究員</dd>
<dt>2018年4月-現在</dt>
<dd>オムロンサイニックエックス株式会社 技術アドバイザ (<a href="call_for_interns_v5.pdf">インターン常時募集中</a>)</dd>
</dl>

<h2>活動歴</h2>
<dl>
<dt>2017年8月</dt>
<dd>画像の 認識・理解シンポジウム (MIRU) 若手プログラム実行委員</dd>
<dt>2018年6月</dt>
<dd>International Conference on Multimedia Retrieval (ICMR 2018) Publication Co-chairs</dd>
<dt>2018年8月</dt>
<dd>画像の 認識・理解シンポジウム (MIRU) 若手プログラム実行委員長</dd>
<dt>2016年4月-現在</dt>
<dd>コンピュータビジョン勉強会＠関東 幹事</dd>
<dt>2017年6月-現在</dt>
<dd>電子情報通信学会 学会誌編集委員会 編集委員</dd>
<dt>2017年6月-現在</dt>
<dd>電子情報通信学会 パターン認識・メディア理解研究会(PRMU) 専門委員</dd>
<dt>2018年6月-現在</dt>
<dd>電子情報通信学会 パターン認識・メディア理解研究会(PRMU) 幹事補佐</dd>
</dl>

<h2>所属</h2>
<div>
<a href="http://www.i.u-tokyo.ac.jp/">東京大学大学院 情報理工学系研究科</a><br>
<a href="http://www.i.u-tokyo.ac.jp/edu/course/m-i/index.shtml">知能機械情報学専攻</a><br>
<a href="http://www.mi.t.u-tokyo.ac.jp/">原田・牛久研究室</a>
</div>

<h2>連絡先</h2>
<div>
<img src="mailaddr.png" width="201" height="29">
<a href="http://www.twitter.com/losnuevetoros"><img src="tw.png" width="32" height="32"></a>
<a href="http://www.facebook.com/yoshitaka.ushiku"><img src="fb.png" width="32" height="32"></a>
<a href="https://www.linkedin.com/in/losnuevetoros"><img src="li.png" width="38.4" height="32"></a>
<a href="http://www.slideshare.net/YoshitakaUshiku"><img src="ss.png" width="70.4" height="32"></a>
</div>

<h2>論文</h2>
<h3>国際学会 (査読付き)</h3>
<ol>
<li>Kohei Uehara, Antonio Tejero-de-Pablos, Yoshitaka Ushiku, Tatsuya Harada. Visual Question Generation for Class Acquisition of Unknown Objects. The 15th European Conference on Computer Vision (<span class="conf">ECCV</span>), 2018.</li>
<li>Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada. Open Set Domain Adaptation by Backpropagation. The 15th European Conference on Computer Vision (<span class="conf">ECCV</span>), 2018.</li>
<li>Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada. Customized Image Narrative Generation via Interactive Visual Question Generation and Answering. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, Tatsuya Harada. Viewpoint-aware Video Summarization. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada. Neural 3D Mesh Renderer. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (spotlight presentation)</li>
<li>Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, Tatsuya Harada. Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018. (oral presentation)</li>
<li>Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada. Between-class Learning for Image Classification. The 31th IEEE Computer Society Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), 2018..</li>
<li>Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, Kate Saenko. Adversarial Dropout Regularization. The 6th International Conference on Learning Representations (<span class="conf">ICLR</span>), 2018.</li>
<li>Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada. Learning from Between-class Examples for Deep Sound Recognition. The 6th International Conference on Learning Representations (<span class="conf">ICLR</span>), 2018.</li>
<li>Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada. Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture. AAAI Conference on Artificial Intelligence (<span class="conf">AAAI</span>), 2018. (oral presentation)</li>
<li>Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Alternating Circulant Random Features for Semigroup Kernels. AAAI Conference on Artificial Intelligence (<span class="conf">AAAI</span>), 2018.</li>
<li>Masatoshi Hidaka, Yuichiro Kikura, Yoshitaka Ushiku, Tatsuya Harada. WebDNN: Fastest DNN Execution Framework on Web Browser. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), Open Source Software Competition, pp.1213-1216, 2017.</li>
<li>Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada. Spatio-temporal Person Retrieval via Natural Language Queries. IEEE International Conference on Computer Vision (<span class="conf">ICCV</span>), 2017.</li>
<li>Qishen Ha, Kohei Watanabe, Takumi Karasawa, Yoshitaka Ushiku, Tatsuya Harada. MFNet: Towards Real-Time Semantic Segmentation for Autonomous Vehicles with Multi-Spectral Scenes. IEEE/RSJ International Conference on Intelligent Robots and Systems (<span class="conf">IROS</span>), 2017.</li>
<li>Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric Tri-training for Unsupervised Domain Adaptation. International Conference on Machine Learning (<span class="conf">ICML</span>), pp.2988-2997, 2017.</li>
<li>Kuniaki Saito, Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. DualNet: Domain-Invariant Network for Visual Question Answering. IEEE International Conference on Multimedia and Expo (<span class="conf">ICME</span>), pp.829-834, 2017. (oral presentation)</li>
<li>Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset. British Machine Vision Conference (<span class="conf">BMVC</span>), pp.53.1-53.12, 2016.</li>
<li>Yoshitaka Ushiku, Masataka Yamaguchi, Yusuke Mukuta, and Tatsuya Harada. Common subspace for model and similarity: Phrase learning for caption generation from images. IEEE International Conference on Computer Vision (<span class="conf">ICCV</span>), pp.2668-2676, 2015. (acceptance rate: 30.9%)</li>
<li>Yoshitaka Ushiku, Masatoshi Hidaka, and Tatsuya Harada. Three guidelines of online learning for large-scale visual recognition. IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), pp.3574-3581, 2014. (acceptance rate: 29.9%)</li>
<li>Asako Kanezaki, Shogo Inaba, Yoshitaka Ushiku, Yukihiko Yamashita, Hiroaki Muraoka, Yasuo Kuniyoshi, and Tatsuya Harada. Hard negative classes for multiple object detection. IEEE International Conference on Robotics and Automation (<span class="conf">ICRA</span>), pp.3066-3073, 2014.</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Efficient Image Annotation for Automatic Sentence Generation. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), pp.549-558, 2012. (full paper, acceptance rate: 20.2%)</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Understanding Images with Natural Sentences. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), Multimedia Grand Challenge, pp.679-682, 2011. (<span class="mention">Special Prize on the Best Application of a Theoretical Framework</span>) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/ACMMM2011_MGC_ushiku.pdf">pdf</a>]</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Automatic Sentence Generation from Images. ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), pp.1533-1536, 2011. (short, acceptance rate: usually 30%) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/ACMMM2011_ushiku.pdf">pdf</a>]</li>
<li>Tatsuya Harada, Yoshitaka Ushiku, Yuya Yamashita, and Yasuo Kuniyoshi. Discriminative Spatial Pyramid. IEEE Conference on Computer Vision and Pattern Recognition (<span class="conf">CVPR</span>), pp.1617-1624, 2011. (acceptance rate: 26.4%) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/CVPR2011_harada.pdf">pdf</a>]</li>
<li>Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Improvement of Image Similarity Measures for Image Browsing and Retrieval Via Latent Space Learning between Images and Long Texts. IEEE International Conference on Image Processing (<span class="conf">ICIP</span>), pp.2365-2368, 2010. [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2010/ICIP2010_ushiku.pdf">pdf</a>]</li>
</ol>
<h3>国際学会 (査読無し、ワークショップ)</h3>
<ol>
<li>Kuniaki Saito, Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Deep Modality Invariant Adversarial Network for Shared Representation Learning. The 16th International Conference on Computer Vision Workshop on Transferring and Adapting Source Knowledge in Computer Vision (<span class="conf">ICCV</span>, Workshop), 2017.</li>
<li>Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada. Spatial-Temporal Weighted Pyramid using Spatial Orthogonal Pooling. The 16th International Conference on Computer Vision Workshop on Compact and Efficient Feature Representation and Learning in Computer Vision (<span class="conf">ICCV</span>, Workshop), 2017.</li>
<li>Takumi Karasawa, Kohei Watanabe, Qishen Ha, Antonio Tejero-De-Pablos, Yoshitaka Ushiku, Tatsuya Harada. Multispectral Object Detection for Autonomous Vehicles. The 25th Annual ACM International Conference on Multimedia (<span class="conf">ACMMM</span>), 2017, (workshop).</li>
<li>Yoshitaka Ushiku, Hiroshi Muraoka, Sho Inaba, Teppei Fujisawa, Koki Yasumoto, Naoyuki Gunji, Takayuki Higuchi, Yuko Hara, Tatsuya Harada, and Yasuo Kuniyoshi. ISI at ImageCLEF 2012: Scalable System for Image Annotation. the 3rd Conference and Labs of the Evaluation Forum (<span class="conf">CLEF</span> 2012), pp.1-12, 2012.</li>
</ol>
<h3>技術報告</h3>
<ol>
<li>Andrew Shin, Yoshitaka Ushiku, and Tatsuya Harada. The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA). arXiv, 1609.06657, 2016.</li>
</ol>

<h3>国内論文誌 (査読付き)</h3>
<ol>
<li>牛久祥孝，原田達也，國吉康夫. 画像・長文からの潜在空間獲得による画像間類似度の改善. 情報処理学会論文誌 (IPSJ), Vol.52, No.12, pp.3496-3503, 2011. [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/IPSJ2011_ushiku.pdf">pdf</a>]</li>
</ol>
<h3>国内学会 (査読付き)</h3>
<ol>
<li>Ryuei Murata, Akisato Kimura, Yoshitaka Ushiku, Takayoshi Yamashita, Yuji Yamauchi, and Hironobu Fujiyoshi. Online Learning Based on Mondrian Forests in Parallel Distributed Processing. 画像の認識・理解シンポジウム (MIRU), OS2-05, 2016.</li>
<li>牛久祥孝, 原田達也, 國吉康夫. キーフレーズ推定と文法モデルによる画像説明文生成. 画像の認識・理解シンポジウム (MIRU), IS2-05(OS3-01), 2012. (長尾賞(最優秀賞)候補論文)</li>
<li>牛久祥孝, 原田達也, 國吉康夫. 画像・文章間の類似度学習による画像説明文の自動生成. 画像の認識・理解シンポジウム (MIRU), pp.365-372, 2011. (<span class="mention">MIRU2011インタラクティブセッション賞</span>) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/MIRU2011_ushiku.pdf">pdf</a>]</li>
<li>牛久祥孝, 原田達也, 國吉康夫. 画像・長文からの潜在空間獲得による画像間類似度の改善. 画像の 認識・理解シンポジウム (MIRU), pp.1153-1160, 2010. [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2010/MIRU2010_ushiku.pdf">pdf</a>]</li>
</ol>
<h3>国内学会 (査読無し)</h3>
<ol>
<li>川口維文, 牛久祥孝, Anna Zhu, 内田誠一. Show, Read with Attention, and Tell.画像の認識・理解シンポジウム (MIRU), PS3-28, 2017.</li>
<li>村田隆英, 木村昭悟, 牛久祥孝, 山下隆義, 山内悠嗣, 藤吉弘亘. 教師あり学習の導入によるMondrian Forestsの効率化. 電子情報通信学会 パターン認識・メディア理解研究会（PRMU）, pp.191-196, 2016.</li>
<li>牛久祥孝, 木村昭悟, 柏野邦夫. 時刻に基づく画像検索のための反復構造の自動抽出. 画像の認識・理解シンポジウム (MIRU), SS4-26, 2015.</li>
<li>郡司直之, 樋口貴之, 安本晃基, 村岡宏是, 牛久祥孝, 原田達也, 國吉康夫. 数の画像特徴とPassive-Aggressiveを用いたfine-grained classification. 電子情報通信学会 パターン認識・メディア理解研究会（PRMU）, pp.25-30, 2013.</li>
<li>金崎朝子, 稲葉翔, 牛久祥孝, 山下裕也, 村岡宏是, 原田達也, 國吉康夫. 大規模画像データセットを用いたマルチクラス物体検出器の同時学習 物体毎に特化した負例クラスの導入. 電子情報通信学会 パターン認識・メディア理解研究会（PRMU）, pp.105-112, 2012.</li>
<li>牛久祥孝, 山下裕也, 井村純, 中山英樹, 原田達也, 國吉康夫. 複数画像特徴とクラスラベルの相関に着目した距離計量による大規模画像分類. 電子情報通信学会 パターン認識・メディア理解研究会（PRMU）, pp.1-6, 2011. (<span class="mention">2010PRMU研究奨励賞</span>) [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2011/PRMU2011_ushiku.pdf">pdf</a>]</li>
<li>牛久祥孝, 中山英樹, 原田達也, 國吉康夫. Web画像と文章の大域的特徴から得る潜在的意味に基づくデータ検索　Web上での一般画像認識実現への新たなアプローチを目指して. 電子情報通信学会 パターン認識・メディア理解研究会（PRMU）, pp.45-50, 2009. [<a href="http://www.isi.imi.i.u-tokyo.ac.jp/publication/2009/PRMU2009_ushiku.pdf">pdf</a>]</li>
</ol>

<h2>解説記事・書籍</h2>
<ol>
<li>【解説記事】牛久祥孝. TBA. 光学. TBA.</li>
<li>【解説記事ゲストエディタ】牛久祥孝. 視覚・言語融合の最前線. 映像情報メディア学会誌. TBA.</li>
<li>【解説記事】牛久祥孝. 画像・動画キャプション生成. 映像情報メディア学会誌. TBA.</li>
<li>【書籍】米谷竜, 斎藤英雄編著, 池畑諭, 牛久祥孝, 内山英昭, 内海ゆづ子, 小野峻佑, 片岡裕雄, 金崎朝子, 川西康友, 齋藤真樹, 櫻田健, 高橋康輔, 松井勇佑. コンピュータビジョン―広がる要素技術と応用. 未来へつなぐデジタルシリーズ, 37巻, 共立出版, 2018.
<li>【解説記事】牛久祥孝. 私のブックマーク 視覚と自然言語の融合研究. 人工知能:人工知能学会誌. Vol.32, No.1, pp.136-143, 2016.</li>
<li>【解説記事】牛久祥孝. ACM Multimedia 参加報告. 情報処理. Vol.56, No.3, p.286, 2015.</li>
</ol>

<h2>基調講演・招待講演</h2>
<ol>
<li>【招待講演】牛久祥孝. TBA. 人工知能学会 言語・音声理解と対話処理研究会(SIG-SLUD), TBA.</li>
<li>【招待講演】牛久祥孝. 敵対的学習による動画生成とドメイン適応. 人工知能学会 第76回人工知能セミナー, 2018/8/27.</li>
<li>【招待講演】牛久祥孝. Deep Learningによる視覚・言語融合の最前線. 精密工学会 画像応用技術専門委員会(IAIP), 2018/7/13.</li>
<li>【招待講演】牛久祥孝. 視覚と言葉をつなげる技術. 情報処理学会 IPSJ-ONE, 2018/3/15.</li>
<li>【招待講演】牛久祥孝. Deep Learningによる視覚・言語融合の最前線. 情報処理学会 コンピュータビジョンとイメージメディア研究会(CVIM), 2018/3/2.</li>
<li>【招待講演】牛久祥孝. Deep Learningによる視覚・言語融合の最前線. 映像情報メディア学会 冬季大会, 2017/12/13.</li>
<li>【招待講演】牛久祥孝. Deep Learningによる視覚・言語融合の最前線. 画像符号化シンポジウム(PCSJ) / 映像メディア処理シンポジウム(IMPS), 2017/11/22.</li>
<li>【招待講演】牛久祥孝. Deep Learningによる視覚・言語融合の最前線. 中部大学 藤吉弘亘教授 総監修 深層学習の基礎と最新動向～画像認識・音声認識・自然言語処理による深層学習とその融合、生成、強化学習, 2017/11/21.</li>
<li>【基調講演】Yoshitaka Ushiku. Frontiers of Vision and Language: Bridging Images and Texts by Deep Learning. Workshop of Machine Learning under International Conference on Document Analysis and Recognition, 2017/11/11.</li>
<li>【招待講演】Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric Tri-training for Unsupervised Domain Adaptation. International Conference on Machine Learning. 第20回画像の認識・理解シンポジウム, 2017/08/10.</li>
<li>【招待講演】牛久祥孝. Deep Learning による視覚×言語融合の最前線. ABEJA Technopreneur College, 2017/06/30.</li>
<li>【招待講演】Yoshitaka Ushiku. Recognize, Describe, and Generate: Introduction of Recent Work at MIL. GPU Technology Conference, 2017/05/11.</li>
<li>【招待講演】牛久祥孝. Deep Learning による視覚×言語融合の最前線. 電子情報通信学会総合大会 企画講演セッション「もっと知りたい！ Deep Learning 〜基礎から活用ノウハウ，応用まで〜」, 2017/03/22.</li>
<li>【招待講演】牛久祥孝. 視覚×言語の最前線. ステアラボ人工知能シンポジウム, 2017/03/12.</li>
<li>【招待講演】牛久祥孝. 画像キャプションの自動生成. 第16回全脳アーキテクチャ勉強会～人工知能は意味をどう獲得するのか～＠リクルートテクノロジーズ, 2016/10/12.</li>
<li>【招待講演】Yoshitaka Ushiku, Masataka Yamaguchi, Yusuke Mukuta, and Tatsuya Harada. Common subspace for model and similarity: Phrase learning for caption generation from images. 第19回画像の認識・理解シンポジウム, 2016/08/02.</li>
<li>【招待講演】牛久祥孝. 画像キャプションの自動生成. 第19回画像の認識・理解シンポジウム (チュートリアルセッション), 2016/08/01.</li>
<li>【招待講演】牛久祥孝. 画像キャプションの自動生成. 第3回ステアラボ人工知能セミナー＠千葉工業大学 (スカイツリータウン), 2016/06/29.</li>
<li>【招待講演】牛久祥孝. 画像キャプションの自動生成. 人工知能セミナー第7回 「自然言語処理のＡＩの最新動向」＠産総研, 2016/06/21.</li>
<li>【招待講演】牛久祥孝. 画像キャプションの自動生成. 確率場と深層学習に関する第1回CRESTシンポジウム＠早稲田大学, 2016/01/13.</li>
<li>【招待講演】Yoshitaka Ushiku, Tatsuya Harada, and Yasuo Kuniyoshi. Efficient Image Annotation for Automatic Sentence Generation. Greater Tokyo Area Multimedia/Vision Workshop, 2012/08/30.</li>
</ol>

<h2>受賞およびコンペティション</h2>
<ol>
<li>2017. <span class="award">Honorable Mention</span>. ACM Multimedia Open Source Software Competition.</li>
<li>2016. <span class="award">First place</span> in the abstract image task. Visual Question Answering Challenge 2016.</li>
<li>2012. <span class="award">First place</span> in the fine-grained classification task, <span class="award">second place</span> in the classification task. Large Scale Visual Recognition Challenge 2012 (ILSVRC2012).</li>
<li>2011. <span class="award">Special Prize on the Best Application of a Theoretical Framework</span>. ACM Mutlimedia Grand Challenge.</li>
<li>2011. <span class="award">Third place</span> in the classification task, <span class="award">second place</span> in the detection task. Large Scale Visual Recognition Challenge 2011 (ILSVRC2011).</li>
<li>2011. <span class="award">インタラクティブセッション賞</span>. 画像の認識理解シンポジウム (MIRU2011).</li>
<li>2010. <span class="award">研究奨励賞</span>. 電子情報通信学会パターン認識・マルチメディア理解研究会 (PRMU).</li>
<li>2010. <span class="award">Third place</span>. Large Scale Visual Recognition Challenge 2010 (ILSVRC2010).</li>
</ol>

</body>
</html>
